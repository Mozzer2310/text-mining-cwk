{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBWQm3bieJBI",
        "outputId": "f5e2c83d-b1d1-4e27-f23f-68e4bec4093f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"dataset-org/dialog_re\")"
      ],
      "metadata": {
        "id": "0IY6zUtfeQrQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In-oJRG1esYh",
        "outputId": "d92370fb-9e47-4a37-e6f8-cb8561a47e03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['dialog', 'relation_data'],\n",
              "        num_rows: 1073\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['dialog', 'relation_data'],\n",
              "        num_rows: 357\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['dialog', 'relation_data'],\n",
              "        num_rows: 358\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4gLnJspgArU",
        "outputId": "73a90eab-d1bd-45f0-ab42-0834b1abefa7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dialog': [\"Speaker 1: It's been an hour and not one of my classmates has shown up! I tell you, when I actually die some people are gonna get seriously haunted!\",\n",
              "  'Speaker 2: There you go! Someone came!',\n",
              "  \"Speaker 1: Ok, ok! I'm gonna go hide! Oh, this is so exciting, my first mourner!\",\n",
              "  'Speaker 3: Hi, glad you could come.',\n",
              "  'Speaker 2: Please, come in.',\n",
              "  \"Speaker 4: Hi, you're Chandler Bing, right? I'm Tom Gordon, I was in your class.\",\n",
              "  'Speaker 2: Oh yes, yes... let me... take your coat.',\n",
              "  \"Speaker 4: Thanks... uh... I'm so sorry about Ross, it's...\",\n",
              "  'Speaker 2: At least he died doing what he loved... watching blimps.',\n",
              "  'Speaker 1: Who is he?',\n",
              "  'Speaker 2: Some guy, Tom Gordon.',\n",
              "  \"Speaker 1: I don't remember him, but then again I touched so many lives.\",\n",
              "  'Speaker 3: So, did you know Ross well?',\n",
              "  \"Speaker 4: Oh, actually I barely knew him. Yeah, I came because I heard Chandler's news. D'you know if he's seeing anyone?\",\n",
              "  'Speaker 3: Yes, he is. Me.',\n",
              "  'Speaker 4: What? You... You... Oh! Can I ask you a personal question? Ho-how do you shave your beard so close?',\n",
              "  \"Speaker 2: Ok Tommy, that's enough mourning for you! Here we go, bye bye!!\",\n",
              "  'Speaker 4: Hey, listen. Call me.',\n",
              "  'Speaker 2: Ok!'],\n",
              " 'relation_data': {'x': ['Speaker 2',\n",
              "   'Speaker 2',\n",
              "   'Speaker 4',\n",
              "   'Speaker 4',\n",
              "   'Speaker 4',\n",
              "   'Speaker 1'],\n",
              "  'y': ['Chandler Bing',\n",
              "   'Speaker 4',\n",
              "   'Tom Gordon',\n",
              "   'Speaker 2',\n",
              "   'Tommy',\n",
              "   'Tommy'],\n",
              "  'x_type': ['PER', 'PER', 'PER', 'PER', 'PER', 'PER'],\n",
              "  'y_type': ['PER', 'PER', 'PER', 'PER', 'PER', 'PER'],\n",
              "  'r': [['per:alternate_names'],\n",
              "   ['per:alumni'],\n",
              "   ['per:alternate_names'],\n",
              "   ['per:alumni', 'per:positive_impression'],\n",
              "   ['per:alternate_names'],\n",
              "   ['unanswerable']],\n",
              "  'rid': [[30], [4], [30], [4, 1], [30], [37]],\n",
              "  't': [[''], [''], [''], ['', 'call me'], [''], ['']]}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example in ds['train']:\n",
        "    # Create the document for each example\n",
        "    document = ' '.join(example['dialog'])"
      ],
      "metadata": {
        "id": "egOZYdRki-Z5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "kZlIbVR6jgda",
        "outputId": "af50d6b6-ffb6-4178-d5e3-a288ec9db66f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Speaker 1: Buon Giorno, Bella Phoebe! Speaker 2: Oh, Paolo, hi, what are you doing here? Speaker 1: Uh, Racquela tell me you massage, eh? Speaker 2: Well, Racquela's right, yeah! Speaker 2: Oh, okay, I don't know what you just said, so let's get started. Speaker 3, Speaker 4: Hey Phoebe! Speaker 5: Hi Pheebs! Speaker 6: Pheebs! Speaker 2: Fine! Speaker 7: Phoebe, what's the matter? Speaker 2: Nothing, I'm sorry, I'm just, I'm out of sorts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTLlomoCp2jn",
        "outputId": "93bdbaeb-8166-4ed8-8eaa-a22467630d04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_sentence_embedding(text):\n",
        "    doc = nlp(text)\n",
        "    return doc.vector  # Mean of all token vectors\n",
        "\n",
        "# Example usage\n",
        "sentence_vectors = []\n",
        "\n",
        "for example in ds['train']:\n",
        "    document = ' '.join(example['dialog'])  # Merge dialogue\n",
        "    sentence_vector = get_sentence_embedding(document)  # Convert to embedding\n",
        "    sentence_vectors.append(sentence_vector)\n",
        "\n",
        "print(np.array(sentence_vectors).shape)  # Check dimensions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfaqolUlnqPK",
        "outputId": "7113883e-6983-4095-abb9-7cf1a6f8a1e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1073, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Extract relation labels\n",
        "relation_labels = [example['relation_data']['r'][0][0] for example in ds['train']]\n",
        "\n",
        "# Convert relation labels to numbers\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(relation_labels)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentence_vectors, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "AxqyrY6frJWl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxsMRLi7rSzS",
        "outputId": "84772dc5-3f1d-4891-934a-85bfb4f95992"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         3\n",
            "           1       0.20      0.20      0.20         5\n",
            "           2       0.42      0.78      0.54        80\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.00      0.00         4\n",
            "           5       0.00      0.00      0.00         4\n",
            "           6       1.00      0.50      0.67         4\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.33      0.20      0.25        15\n",
            "          11       0.21      0.17      0.19        18\n",
            "          12       0.00      0.00      0.00         3\n",
            "          14       0.00      0.00      0.00         0\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         9\n",
            "          17       1.00      0.50      0.67         2\n",
            "          18       0.00      0.00      0.00         3\n",
            "          19       0.00      0.00      0.00         2\n",
            "          20       0.10      0.06      0.07        18\n",
            "          21       0.50      0.20      0.29         5\n",
            "          23       0.00      0.00      0.00         6\n",
            "          24       0.25      0.14      0.18         7\n",
            "          25       0.25      0.06      0.10        16\n",
            "          26       0.00      0.00      0.00         2\n",
            "          27       0.50      0.50      0.50         2\n",
            "\n",
            "    accuracy                           0.36       215\n",
            "   macro avg       0.20      0.14      0.15       215\n",
            "weighted avg       0.28      0.36      0.29       215\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_relation(text, model, label_encoder):\n",
        "    sentence_vector = get_sentence_embedding(text)  # Convert to vector\n",
        "    predicted_label = model.predict([sentence_vector])[0]  # Predict label\n",
        "    return label_encoder.inverse_transform([predicted_label])[0]  # Convert back to text\n",
        "\n",
        "# Example usage\n",
        "new_text = \"Ross and Chandler were classmates in college.\"\n",
        "predicted_relation = predict_relation(new_text, clf, label_encoder)\n",
        "print(f\"Predicted relation: {predicted_relation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YJ6x4rArdtT",
        "outputId": "8a3e2a26-78c9-48bb-b7e2-06dcb9a61b95"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted relation: unanswerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"dataset-org/dialog_re\")\n",
        "\n",
        "# Load spaCy model with GloVe embeddings\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "embedding_dim = nlp(\"word\").vector.shape[0]  # Get vector size\n",
        "\n",
        "# Function to extract word vector for an entity\n",
        "def get_entity_embedding(entity):\n",
        "    doc = nlp(entity)\n",
        "    vectors = [token.vector for token in doc if token.has_vector]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(embedding_dim)  # Return zero vector if no embedding found\n",
        "\n",
        "# Function to create feature vectors\n",
        "def create_feature_vector(dialog, entity1, entity2):\n",
        "    sentence_vector = np.mean([token.vector for token in nlp(dialog) if token.has_vector], axis=0)\n",
        "\n",
        "    if sentence_vector is None or np.isnan(sentence_vector).any():\n",
        "        sentence_vector = np.zeros(embedding_dim)\n",
        "\n",
        "    entity1_vector = get_entity_embedding(entity1)\n",
        "    entity2_vector = get_entity_embedding(entity2)\n",
        "\n",
        "    # Concatenate sentence embedding with entity embeddings\n",
        "    feature_vector = np.concatenate((sentence_vector, entity1_vector, entity2_vector))\n",
        "    return feature_vector\n",
        "\n",
        "# Prepare data for training\n",
        "X, y = [], []\n",
        "\n",
        "for example in ds[\"train\"]:\n",
        "    dialog = \" \".join(example[\"dialog\"])  # Combine dialog lines into one text\n",
        "    for i in range(len(example[\"relation_data\"][\"x\"])):\n",
        "        entity1 = example[\"relation_data\"][\"x\"][i]\n",
        "        entity2 = example[\"relation_data\"][\"y\"][i]\n",
        "        relation = example[\"relation_data\"][\"r\"][i][0]  # Take first relation (if multiple exist)\n",
        "\n",
        "        feature_vector = create_feature_vector(dialog, entity1, entity2)\n",
        "        X.append(feature_vector)\n",
        "        y.append(relation)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Filter out classes with fewer than 2 samples\n",
        "class_counts = Counter(y)\n",
        "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
        "X_filtered = [X[i] for i in range(len(y)) if y[i] in valid_classes]\n",
        "y_filtered = [y[i] for i in range(len(y)) if y[i] in valid_classes]\n",
        "X = np.array(X_filtered)\n",
        "y = np.array(y_filtered)\n",
        "\n",
        "# Adjust k_neighbors for SMOTE\n",
        "min_samples = min(Counter(y).values())\n",
        "target_k_neighbors = max(1, min(min_samples - 1, 5))\n",
        "smote = SMOTE(random_state=42, k_neighbors=target_k_neighbors)\n",
        "\n",
        "# Apply SMOTE only if we have enough samples\n",
        "if min_samples > 1:\n",
        "    X, y = smote.fit_resample(X, y)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Train an SVM model with class weighting\n",
        "clf = SVC(kernel='linear', class_weight='balanced')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# Inference function\n",
        "def predict_relation(dialog, entity1, entity2):\n",
        "    vec = create_feature_vector(dialog, entity1, entity2).reshape(1, -1)\n",
        "    return clf.predict(vec)[0]\n",
        "\n",
        "# Example usage\n",
        "test_dialog = \"Speaker 1: Hey, do you know Chandler Bing? Speaker 2: Oh yes, he is my classmate.\"\n",
        "print(\"Predicted Relation:\", predict_relation(test_dialog, \"Chandler Bing\", \"classmate\"))"
      ],
      "metadata": {
        "id": "usXddCxGma8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset containing dialogues and relation annotations\n",
        "ds = load_dataset(\"dataset-org/dialog_re\")\n",
        "\n",
        "# Load the pre-trained spaCy model with GloVe word embeddings\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "embedding_dim = nlp(\"word\").vector.shape[0]  # Get the dimensionality of word embeddings\n",
        "\n",
        "# Function to extract the word vector representation for an entity\n",
        "def get_entity_embedding(entity):\n",
        "    doc = nlp(entity)  # Process entity with spaCy\n",
        "    vectors = [token.vector for token in doc if token.has_vector]  # Extract word vectors if available\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)  # Return the average vector of the entity\n",
        "    else:\n",
        "        return np.zeros(embedding_dim)  # Return a zero vector if no embedding is found\n",
        "\n",
        "# Function to create a feature vector for a given dialogue and entity pair\n",
        "def create_feature_vector(dialog, entity1, entity2):\n",
        "    sentence_vector = np.mean([token.vector for token in nlp(dialog) if token.has_vector], axis=0)  # Compute sentence embedding\n",
        "\n",
        "    # Handle cases where the sentence embedding is empty\n",
        "    if sentence_vector is None or np.isnan(sentence_vector).any():\n",
        "        sentence_vector = np.zeros(embedding_dim)\n",
        "\n",
        "    entity1_vector = get_entity_embedding(entity1)  # Get embedding for entity 1\n",
        "    entity2_vector = get_entity_embedding(entity2)  # Get embedding for entity 2\n",
        "\n",
        "    # Concatenate the sentence embedding with both entity embeddings to form the feature vector\n",
        "    feature_vector = np.concatenate((sentence_vector, entity1_vector, entity2_vector))\n",
        "    return feature_vector\n",
        "\n",
        "# Prepare the dataset for training\n",
        "X, y = [], []\n",
        "\n",
        "for example in ds[\"train\"]:\n",
        "    dialog = \" \".join(example[\"dialog\"])  # Combine all dialogue lines into a single text\n",
        "    for i in range(len(example[\"relation_data\"][\"x\"])):\n",
        "        entity1 = example[\"relation_data\"][\"x\"][i]  # Extract the first entity\n",
        "        entity2 = example[\"relation_data\"][\"y\"][i]  # Extract the second entity\n",
        "        relation = example[\"relation_data\"][\"r\"][i][0]  # Extract the first relation label (if multiple exist)\n",
        "\n",
        "        feature_vector = create_feature_vector(dialog, entity1, entity2)  # Generate feature vector\n",
        "        X.append(feature_vector)  # Append feature vector to training data\n",
        "        y.append(relation)  # Append relation label to target labels\n",
        "\n",
        "# Convert feature lists to NumPy arrays for efficient processing\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Filter out classes that have fewer than 2 samples to avoid train-test split issues\n",
        "class_counts = Counter(y)\n",
        "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
        "X_filtered = [X[i] for i in range(len(y)) if y[i] in valid_classes]\n",
        "y_filtered = [y[i] for i in range(len(y)) if y[i] in valid_classes]\n",
        "X = np.array(X_filtered)\n",
        "y = np.array(y_filtered)\n",
        "\n",
        "# Determine the smallest class size to adjust k_neighbors in SMOTE\n",
        "min_samples = min(Counter(y).values())\n",
        "target_k_neighbors = max(1, min(min_samples - 1, 5))  # Ensure k_neighbors is valid\n",
        "smote = SMOTE(random_state=42, k_neighbors=target_k_neighbors)\n",
        "\n",
        "# Apply SMOTE only if there are enough samples to generate synthetic data\n",
        "if min_samples > 1:\n",
        "    X, y = smote.fit_resample(X, y)\n",
        "\n",
        "# Split data into training and testing sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Train a Support Vector Machine (SVM) model with class weighting to handle imbalanced data\n",
        "clf = SVC(kernel='linear', class_weight='balanced')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# Function to predict the relation between two entities given a dialogue\n",
        "def predict_relation(dialog, entity1, entity2):\n",
        "    vec = create_feature_vector(dialog, entity1, entity2).reshape(1, -1)  # Convert input into a feature vector\n",
        "    return clf.predict(vec)[0]  # Return predicted relation\n",
        "\n",
        "# Example usage\n",
        "test_dialog = \"Speaker 1: Hey, do you know Chandler Bing? Speaker 2: Oh yes, he is my classmate.\"\n",
        "print(\"Predicted Relation:\", predict_relation(test_dialog, \"Chandler Bing\", \"classmate\"))"
      ],
      "metadata": {
        "id": "ksKQ66Xyxza7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset containing dialogues and their relation annotations\n",
        "ds = load_dataset(\"dataset-org/dialog_re\")\n",
        "\n",
        "# Load the pre-trained spaCy model with GloVe word embeddings\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Function to extract the word vector representation for an entity\n",
        "def get_entity_embedding(entity):\n",
        "    doc = nlp(entity)  # Process entity with spaCy\n",
        "    vectors = [token.vector for token in doc if token.has_vector]  # Extract word vectors if available\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)  # Return the average vector of the entity\n",
        "    else:\n",
        "        return np.zeros(nlp(\"word\").vector.shape[0])  # Return a zero vector if no embedding is found\n",
        "\n",
        "# Function to create a feature vector for a given dialogue and entity pair\n",
        "def create_feature_vector(dialog, entity1, entity2):\n",
        "    sentence_vector = np.mean([token.vector for token in nlp(dialog) if token.has_vector], axis=0)  # Compute sentence embedding\n",
        "\n",
        "    # Handle cases where the sentence embedding is empty\n",
        "    if sentence_vector is None or np.isnan(sentence_vector).any():\n",
        "        sentence_vector = np.zeros(nlp(\"word\").vector.shape[0])\n",
        "\n",
        "    entity1_vector = get_entity_embedding(entity1)  # Get embedding for entity 1\n",
        "    entity2_vector = get_entity_embedding(entity2)  # Get embedding for entity 2\n",
        "\n",
        "    # Concatenate the sentence embedding with both entity embeddings to form the feature vector\n",
        "    feature_vector = np.concatenate((sentence_vector, entity1_vector, entity2_vector))\n",
        "    return feature_vector\n",
        "\n",
        "# Prepare the dataset for training\n",
        "X, y = [], []\n",
        "\n",
        "for example in ds[\"train\"]:\n",
        "    dialog = \" \".join(example[\"dialog\"])  # Combine all dialogue lines into a single text\n",
        "    for i in range(len(example[\"relation_data\"][\"x\"])):\n",
        "        entity1 = example[\"relation_data\"][\"x\"][i]  # Extract the first entity\n",
        "        entity2 = example[\"relation_data\"][\"y\"][i]  # Extract the second entity\n",
        "        relation = example[\"relation_data\"][\"r\"][i][0]  # Extract the first relation label (if multiple exist)\n",
        "\n",
        "        feature_vector = create_feature_vector(dialog, entity1, entity2)  # Generate feature vector\n",
        "        X.append(feature_vector)  # Append feature vector to training data\n",
        "        y.append(relation)  # Append relation label to target labels\n",
        "\n",
        "# Convert feature lists to NumPy arrays for efficient processing\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split data into training and testing sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Train a Support Vector Machine (SVM) model with class weighting to handle imbalanced data\n",
        "clf = SVC(kernel='linear', class_weight='balanced')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# Function to predict the relation between two entities given a dialogue\n",
        "def predict_relation(dialog, entity1, entity2):\n",
        "    vec = create_feature_vector(dialog, entity1, entity2).reshape(1, -1)  # Convert input into a feature vector\n",
        "    return clf.predict(vec)[0]  # Return predicted relation\n",
        "\n",
        "# Example usage\n",
        "test_dialog = \"Speaker 1: Hey, do you know Chandler Bing? Speaker 2: Oh yes, he is my classmate.\"\n",
        "print(\"Predicted Relation:\", predict_relation(test_dialog, \"Chandler Bing\", \"classmate\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "LyALcTwhz7SC",
        "outputId": "01b0e8d6-3854-41c7-9c6a-c25e2539c02f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-878e90f08aeb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the pre-trained spaCy model with GloVe word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Function to extract the word vector representation for an entity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MpAtNNPbxtZ",
        "outputId": "1f93d684-340a-4e89-ba95-00eeb8c9540d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-26 13:49:13--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-02-26 13:49:13--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-02-26 13:49:14--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1        2%[                    ]  16.83M  4.43MB/s    eta 3m 30s ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22cU4hvXb37M",
        "outputId": "28b30787-e8ac-491d-c28d-e55fe52a1785"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:])\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "glove_path = \"glove.6B.100d.txt\"\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "print(\"'hello' embedding: \", glove_embeddings.get(\"hello\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76pBBsZb7rx",
        "outputId": "b5516273-039b-4bcf-eda7-3839fcfe8687"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'hello' embedding:  ['0.26688' '0.39632' '0.6169' '-0.77451' '-0.1039' '0.26697' '0.2788'\n",
            " '0.30992' '0.0054685' '-0.085256' '0.73602' '-0.098432' '0.5479'\n",
            " '-0.030305' '0.33479' '0.14094' '-0.0070003' '0.32569' '0.22902'\n",
            " '0.46557' '-0.19531' '0.37491' '-0.7139' '-0.51775' '0.77039' '1.0881'\n",
            " '-0.66011' '-0.16234' '0.9119' '0.21046' '0.047494' '1.0019' '1.1133'\n",
            " '0.70094' '-0.08696' '0.47571' '0.1636' '-0.44469' '0.4469' '-0.93817'\n",
            " '0.013101' '0.085964' '-0.67456' '0.49662' '-0.037827' '-0.11038'\n",
            " '-0.28612' '0.074606' '-0.31527' '-0.093774' '-0.57069' '0.66865'\n",
            " '0.45307' '-0.34154' '-0.7166' '-0.75273' '0.075212' '0.57903' '-0.1191'\n",
            " '-0.11379' '-0.10026' '0.71341' '-1.1574' '-0.74026' '0.40452' '0.18023'\n",
            " '0.21449' '0.37638' '0.11239' '-0.53639' '-0.025092' '0.31886' '-0.25013'\n",
            " '-0.63283' '-0.011843' '1.377' '0.86013' '0.20476' '-0.36815' '-0.68874'\n",
            " '0.53512' '-0.46556' '0.27389' '0.4118' '-0.854' '-0.046288' '0.11304'\n",
            " '-0.27326' '0.15636' '-0.20334' '0.53586' '0.59784' '0.60469' '0.13735'\n",
            " '0.42232' '-0.61279' '-0.38486' '0.35842' '-0.48464' '0.30728']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset containing dialogues and their relation annotations\n",
        "ds = load_dataset(\"dataset-org/dialog_re\")\n",
        "\n",
        "\n",
        "# Load the pre-trained spaCy model with GloVe word embeddings\n",
        "nlp = spacy.load(\"glove_vectors\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "7T1iOLvcZZvn",
        "outputId": "b70a415b-134f-4dc4-c0e5-5ce2a198d88b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'glove_vectors'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-86c847a6f390>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the pre-trained spaCy model with GloVe word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove_vectors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'glove_vectors'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"dataset-org/dialog_re\")\n",
        "\n",
        "# Load spaCy tokenizer (without built-in word vectors)\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Function to load GloVe embeddings from file\n",
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]  # Extract word\n",
        "            vector = np.asarray(values[1:], dtype=np.float32)  # Convert to numpy array\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "# Load 100d GloVe embeddings\n",
        "glove_path = \"glove.6B.100d.txt\"\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "# Get embedding size\n",
        "embedding_dim = len(next(iter(glove_embeddings.values())))\n",
        "\n",
        "# Function to get GloVe embedding for a token\n",
        "def get_word_embedding(token):\n",
        "    return glove_embeddings.get(token.lower(), np.zeros(embedding_dim))  # Return zero vector if not found\n",
        "\n",
        "# Function to tokenize and apply embeddings\n",
        "def process_dialog(dialog):\n",
        "    tokenized_dialog = []  # Store tokenized words\n",
        "    word_embeddings = []  # Store corresponding word embeddings\n",
        "\n",
        "    for line in dialog:\n",
        "        doc = nlp(line)  # Tokenize the line\n",
        "        for token in doc:\n",
        "            tokenized_dialog.append(token.text)  # Store tokens\n",
        "            word_embeddings.append(get_word_embedding(token.text))  # Store embeddings\n",
        "\n",
        "    return tokenized_dialog, np.array(word_embeddings)\n",
        "\n",
        "# Select a sample dialog\n",
        "sample_dialog = ds[\"train\"][0][\"dialog\"]\n",
        "\n",
        "# Tokenize and obtain word embeddings\n",
        "tokens, embeddings = process_dialog(sample_dialog)\n",
        "\n",
        "# Display results\n",
        "print(\"Tokens:\", tokens[:10])  # Show first 10 tokens\n",
        "print(\"Word Embedding Shape:\", embeddings.shape)  # Shape should be (num_tokens, 100)\n",
        "\n",
        "# Function to create contextualized embeddings (sentence-level)\n",
        "def get_contextualized_embedding(embeddings, method=\"average\"):\n",
        "    if method == \"average\":\n",
        "        return np.mean(embeddings, axis=0)  # Compute mean across tokens\n",
        "    elif method == \"concat\":\n",
        "        return np.concatenate(embeddings, axis=None)  # Concatenate token embeddings\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported method: choose 'average' or 'concat'.\")\n",
        "\n",
        "# Compute a contextualized representation of the dialog\n",
        "contextualized_embedding = get_contextualized_embedding(embeddings, method=\"average\")\n",
        "\n",
        "print(\"Contextualized Embedding Shape:\", contextualized_embedding.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1-1FyDnfJe2",
        "outputId": "b629be03-4511-4ca0-d423-c2c6b6a61539"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Speaker', '1', ':', 'It', \"'s\", 'been', 'an', 'hour', 'and', 'not']\n",
            "Word Embedding Shape: (309, 100)\n",
            "Contextualized Embedding Shape: (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "\n",
        "# Load dataset\n",
        "ds = load_dataset(\"dataset-org/dialog_re\")\n",
        "\n",
        "# Load spaCy tokenizer (without built-in word vectors)\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Function to load GloVe embeddings from file\n",
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]  # Extract word\n",
        "            vector = np.asarray(values[1:], dtype=np.float32)  # Convert to numpy array\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove_path = \"glove.6B.100d.txt\"\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "# Get embedding size\n",
        "embedding_dim = len(next(iter(glove_embeddings.values())))\n",
        "\n",
        "# Function to get GloVe embedding for a token\n",
        "def get_word_embedding(token):\n",
        "    return glove_embeddings.get(token.lower(), np.zeros(embedding_dim))  # Return zero vector if not found\n",
        "\n",
        "# Function to process dialogue and get average embeddings\n",
        "def process_relation_data(dialog, entities, relations):\n",
        "    relation_embeddings = []  # Store feature vectors\n",
        "    relation_labels = []  # Store relation labels\n",
        "\n",
        "    for i in range(len(entities[\"x\"])):\n",
        "        entity1 = entities[\"x\"][i]\n",
        "        entity2 = entities[\"y\"][i]\n",
        "        relation = relations[i][0]  # Take first relation if multiple exist\n",
        "\n",
        "        # Concatenate dialog into one text\n",
        "        full_text = \" \".join(dialog)\n",
        "        doc = nlp(full_text)  # Tokenize text\n",
        "\n",
        "        # Get embeddings for all words\n",
        "        word_vectors = [get_word_embedding(token.text) for token in doc]\n",
        "\n",
        "        if word_vectors:  # Ensure we have embeddings\n",
        "            avg_embedding = np.mean(word_vectors, axis=0)  # Average word embeddings\n",
        "        else:\n",
        "            avg_embedding = np.zeros(embedding_dim)  # Default to zero vector if empty\n",
        "\n",
        "        relation_embeddings.append(avg_embedding)\n",
        "        relation_labels.append(relation)\n",
        "\n",
        "    return np.array(relation_embeddings), np.array(relation_labels)\n",
        "\n",
        "# Prepare dataset\n",
        "X, y = [], []\n",
        "for example in ds[\"train\"]:\n",
        "    dialog = example[\"dialog\"]  # Get conversation text\n",
        "    entities = example[\"relation_data\"]  # Get entity pairs\n",
        "    relations = example[\"relation_data\"][\"r\"]  # Get relations\n",
        "\n",
        "    X_sample, y_sample = process_relation_data(dialog, entities, relations)\n",
        "\n",
        "    X.extend(X_sample)\n",
        "    y.extend(y_sample)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Count occurrences of each relation\n",
        "relation_counts = Counter(y)\n",
        "\n",
        "# Remove relations with fewer than 2 samples\n",
        "valid_relations = {relation for relation, count in relation_counts.items() if count >= 2}\n",
        "X_filtered = [X[i] for i in range(len(y)) if y[i] in valid_relations]\n",
        "y_filtered = [y[i] for i in range(len(y)) if y[i] in valid_relations]\n",
        "X = np.array(X_filtered)\n",
        "y = np.array(y_filtered)\n",
        "\n",
        "# Ensure dataset is not empty\n",
        "if len(set(y)) < 2:\n",
        "    raise ValueError(\"Not enough valid classes with at least 2 samples each after filtering.\")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Train a Support Vector Machine (SVM) model\n",
        "clf = SVC(kernel='linear', class_weight='balanced')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# Inference function\n",
        "def predict_relation(dialog):\n",
        "    doc = nlp(dialog)  # Tokenize input text\n",
        "    word_vectors = [get_word_embedding(token.text) for token in doc]\n",
        "\n",
        "    if word_vectors:\n",
        "        avg_embedding = np.mean(word_vectors, axis=0)  # Average word embeddings\n",
        "    else:\n",
        "        avg_embedding = np.zeros(embedding_dim)  # Default zero vector if no valid words\n",
        "\n",
        "    return clf.predict(avg_embedding.reshape(1, -1))[0]  # Predict relation\n",
        "\n",
        "# Example usage\n",
        "test_dialog = \"Speaker 1: Hey, do you know Chandler Bing? Speaker 2: Oh yes, he is my classmate.\"\n",
        "print(\"Predicted Relation:\", predict_relation(test_dialog))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWH7Cin1fQVp",
        "outputId": "c58677fb-2143-4ef8-894a-ad945e15f278"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           precision    recall  f1-score   support\n",
            "\n",
            "   gpe:residents_of_place       0.00      0.00      0.00        10\n",
            "    gpe:visitors_of_place       0.00      0.00      0.00        10\n",
            " org:employees_or_members       0.01      0.11      0.02         9\n",
            "             org:students       0.00      0.00      0.00         1\n",
            "         per:acquaintance       0.05      0.50      0.09         4\n",
            "                  per:age       0.06      0.09      0.07        11\n",
            "      per:alternate_names       0.47      0.03      0.05       266\n",
            "               per:alumni       0.08      0.37      0.13        19\n",
            "                 per:boss       0.04      0.20      0.06        10\n",
            "             per:children       0.12      0.06      0.08        34\n",
            "               per:client       0.05      0.10      0.06        10\n",
            "                per:dates       0.06      0.50      0.10         6\n",
            "per:employee_or_member_of       0.00      0.00      0.00         9\n",
            "              per:friends       0.19      0.06      0.10        79\n",
            "       per:girl/boyfriend       0.23      0.14      0.17        88\n",
            "                per:major       0.00      0.00      0.00         0\n",
            "  per:negative_impression       0.05      0.26      0.08        27\n",
            "             per:neighbor       0.08      0.29      0.13         7\n",
            "               per:origin       0.05      0.50      0.10         4\n",
            "         per:other_family       0.05      0.27      0.08        15\n",
            "              per:parents       0.08      0.12      0.10        34\n",
            "                  per:pet       0.10      0.83      0.18         6\n",
            "   per:place_of_residence       0.02      0.10      0.04        10\n",
            "        per:place_of_work       0.08      0.14      0.10         7\n",
            "  per:positive_impression       0.30      0.04      0.07        81\n",
            "             per:roommate       0.15      0.20      0.17        25\n",
            "     per:schools_attended       0.00      0.00      0.00         1\n",
            "             per:siblings       0.20      0.05      0.09        37\n",
            "               per:spouse       0.16      0.11      0.13        38\n",
            "          per:subordinate       0.00      0.00      0.00         8\n",
            "                per:title       0.00      0.00      0.00        52\n",
            "        per:visited_place       0.00      0.00      0.00        10\n",
            "                per:works       0.15      0.50      0.23        12\n",
            "             unanswerable       0.50      0.02      0.04       260\n",
            "\n",
            "                 accuracy                           0.08      1200\n",
            "                macro avg       0.10      0.16      0.07      1200\n",
            "             weighted avg       0.29      0.08      0.07      1200\n",
            "\n",
            "Predicted Relation: per:acquaintance\n"
          ]
        }
      ]
    }
  ]
}